{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics used in Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In many supervised machine learning statements there are two types of use cases: classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification case, whenever we need to check for accuracy, we follow the techniques like confusion matrix, accuracy score, where we can see what is the true positive rate, what is the recall value, what is the precision value, what is the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regression, there is a slightly different way to check the accuracy of the paticular model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These techniques are:\n",
    "1. R^2 (R-square) and adjusted R^2\n",
    "2. Mean Square Error (MSE) /Root Mean Square Error (RMSE)\n",
    "3. Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$ (R-square)\n",
    "\n",
    "#### It is square of Correlation Coefficient (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large R^2 = 1 - \\frac{SS_{residual}}{SS_{total}} = 1 - \\frac{\\sum(y_i - \\hat y_i)^2}{\\sum(y_i - \\bar y_i)^2}$, where\n",
    "\n",
    "$\\large SS_{residual}$ is hte sum of residual or error (residual or error is the difference between actual value and predicted value of best fit line)\n",
    "\n",
    "$\\large SS_{residual} = \\sum(y_i - \\hat y_i)^2$, where $\\large y$ is actual value and $\\large \\hat y$ is predicted value. We square them because we need to find the difference between the negative values too.\n",
    "\n",
    "$\\large SS_{total}$ is the sum of average total. Average total is the average of all the values (parallel to x axis). We make it as our best fit line and calculate the errors (difference) between actual values and best fit line values.\n",
    "\n",
    "$\\large SS_{total} = \\sum(y_i - \\bar y_i)^2$, where $\\large y$ is an actual value and $\\large \\bar y$ is the average value. We square them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we compute formula $\\large R^2 = 1 - \\frac{SS_{residual}}{SS_{total}}$ , we will receive value between 0 and 1. When the value is closer to 1, that means that the line is best fitted to the model nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Side Note**\n",
    "*Can you get $R^2$ value below 0? Is it possible?*\n",
    "\n",
    "the answer is YES.\n",
    "\n",
    "$R^2$ may be below zero only if the best fit line is worse than the average best fit line.\n",
    "\n",
    "So, what happens in the formula\n",
    "\n",
    "If $\\large SS_{residual}$ is grater than $\\large SS_{total}$ , value will be $> 1$ and 1 - $value > 1 $ will produce negative value.\n",
    "Negative value shows that model is very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large R^2$ is used to check the goodness of the best fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the adjusted $R^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, in linear regression we have \n",
    "\n",
    "$\\large y = m_{0} + m_{1} x_{1}$ (simple linear regression)\n",
    "\n",
    "$\\large y = m_{0} + m_{1} x_{1} + m_{2}x_{2} + m_{3}x_{3} + m_{4}x_{4} $ (multiple linear regression)\n",
    "\n",
    "As we add new independent features, our $R^2$ value usually increases. It happens because when we add a new feature, our formula assigns some value to that feature (coefficient), which decreases our $\\large SS_{residual}$ value. When $\\large SS_{residual}$ decreases, the outcome of the $\\large \\frac{SS_{residual}}{SS_{total}}$ will also decrease. Since $\\large \\frac{SS_{residual}}{SS_{total}}$ will be some small value, our formula $\\large R^2 = 1 - \\frac{SS_{residual}}{SS_{total}}$ value will increase, it will be closer to 1.\n",
    "\n",
    "It always happens when adding new independent features and it will never decrease. The problem is if the feature that we added is not correlated with our target, $R^2$ value will still increase. Basicaly, $R^2$ is not penalizing our newly added features.\n",
    "\n",
    "For that we use **Adjusted $R^2$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large R^2 adjusted = 1 - \\frac{(1-R^2)(N-1)}{N - p - 1}$, where\n",
    "\n",
    "$R^2$ is sample R-squared (which was calculated above)\n",
    "\n",
    "p is numper of predictors (independent features)\n",
    "\n",
    "N is total sample size (size of the dataset, including the output feature)\n",
    "\n",
    "It penalizes attributes that are not correlated.\n",
    "\n",
    "when the number of features increases we subtract it from the total sample and subtract 1 (N - p - 1), this subtraction makes value smaller value. When we multiply in the nominator (1-R^2)(N-1) it will be a bigger number. and after we divide and subtract it from 1, the $R^2$ value will decrease,but only when we try to add the features which are not correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion R^2\n",
    "\n",
    "- Every time you add an independent variable to a model, the **R-squared increases** even if the independent variable is insignificant. It never declines. Whereas **Adjusted R-squared** increases only when independent variable is significant and affects dependent variable.\n",
    "\n",
    "- Adjusted R-squared value is always less than or equal to R-squared value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE, MSE, RMSE Regression Evaluation Metrics\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\large\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$$\\large\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\large\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "#### Comparing these metrics:\n",
    "\n",
    "**MAE** is the easiest to understand, because it's the average error.\n",
    "\n",
    "**MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "\n",
    "**RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "#### *All of these are loss functions, because we want to minimize them.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Confusion Matrix\n",
    "2. FPR - False Positive Rate (Type I Error)\n",
    "3. FNR - False Negative Rate (Type II Error)\n",
    "4. Recall (TPR(True Positive Rate), Sensitivity)\n",
    "5. Precision (Positive Prediction Value)\n",
    "6. Accuracy\n",
    "7. F Beta (F1 score)\n",
    "8. Cohen Kappa\n",
    "9. ROC Curve, AUC Score\n",
    "10. PR Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways how to solve classification problem:\n",
    " - One way through **class labels**\n",
    "   (For example, in binary classification there are two classes A or B, so the output will be A or B. By default the threshold value is 0.5.       Suppose we have a machine learning model, if we predict value higher than 0.5, than it will become a B class, if it's less or equal 0.5 it     will become A class)\n",
    " - other way through **probabilities**\n",
    "   (With probabilities, we also have to find out the class labels. How? We have to select the right threshold value, which is basically \n",
    "    p-value. In probabilities we have ROC curve, AUC score, PR curve)\n",
    "    \n",
    "*Aside Note*, this threshold value in health care sector may descease, for example in case of desease prediction. Suppose a person has cancer or not at that time this threshold value should be chosen in a proper way so that the person who actually has cancer won't be missed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class labels**\n",
    "\n",
    "Suppose we have dataset with 1000 records (binary classification problem)\n",
    "\n",
    "We my have 500 yes and 500 no (50%-50% ratio)\n",
    "\n",
    "Or         600 yes and 400 no (60%-40% ratio)\n",
    "\n",
    "Or         700 yes and 300 no (70%-30% ratio)\n",
    "\n",
    "In this case above it looks like a balanced data set, balanced means that we have almost the same number of yes and no. When we have almost equal number of classes, our algorithm will not get biased based on the maximum(1000) number of output.\n",
    "\n",
    "But in a scenario when we have 800 yes and 200 no (80%-20% ratio)\n",
    "this data set is imbalanced. When we give this dataset to our machine learning algorithm, some of machine learning algorthms will get biased based on the maximum(1000) number of output.\n",
    "\n",
    "**If we have balanced dataset, the types of metrics is used like accuracy. If we have imbalanced dataset we do not consider accuracy, instead we consider recall, precision, and F beta(F1 score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
