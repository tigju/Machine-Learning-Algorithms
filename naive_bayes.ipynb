{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "#### (For binary or multi classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NBC is based on Naive Bayes Theorem\n",
    "\n",
    "$$ \\large P(A|B) = \\frac{P(A|B)\\times P(A)}{P(B)} $$\n",
    "\n",
    "where **P(A|B)** is Probability of A given B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What are the basic assumptions?\n",
    "- Features are independent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the advantages when working with Naive Bayes Classifier?\n",
    "- Works very well with many numbers of features (For example Natural Language Processing, after text pre-processing we end up with thousands of vectors based on number of words in a dictionary).\n",
    "- Works well with large datasets, it will perform fast with respect to time, because Naive Bayes classifier completely works on probability.\n",
    "- It converges faster when training a model, the convergence happens very quickly.\n",
    "- It also performs good with categorical features, as well as with sparce matrices when there are a lot of zeros and ones, for example after Bag Of Words text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Disadvantages\n",
    "- Correlated features affect performance\n",
    "---\n",
    "\n",
    "Suppose you there are 5000 features after some NLP pre-processing technics and there are many features within the 5000 which are highly correlated, it will affect performance due to probabilities in Naive Bayes. Probability is a main key concept inside it. Performance is in impact because when calculating the probability of one feature with respect to another feature, and if they are highly correlated they are selected twice in the model, overinflating their importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Whether feature scaling is required?\n",
    "- No. Naive Bayes works based on probability, no feature scaling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Impact of Missing Values\n",
    "- Naive Bayes can handle missing values\n",
    "---\n",
    "\n",
    "Attributes are handled separately by the algorithm at both model construction time and prediction time. If a data instance has a missing value for an attribute, it will be ignored while preparing the model and ignored when probability is calculated for an class value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Impact of Outliers\n",
    "- It is usually robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Problem Statements can be solved using Naive Bayes\n",
    "- Sentiment Analysis\n",
    "- Spam classification\n",
    "- twitter sentiment analysis\n",
    "- document categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Formula use in Naive Bayes Classifier\n",
    "\n",
    "$$ \\large P(A|B) = \\frac{P(A|B)\\times P(A)}{P(B)} $$\n",
    "\n",
    "where **P(A|B)** Probability of A given B, when A is already given\n",
    "\n",
    "Suppose we have dataset:\n",
    "independent features:\n",
    "x = {$ x_{1}, x_{2}, x_{3} ... x_{n}$}\n",
    "and dependent feature:\n",
    "y = {y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can convert formula like this:\n",
    "\n",
    "$$\\large P(y|x_{1}, x_{2}, x_{3}...x_{n}) = \\frac{P(x_{1}|y) \\times P(x_{2}|y) ... P(x_{n}|y)\\times P(y)}{P(x_{1})\\times P(x_{2})\\times P(x_{3}) ... P(x_{n})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we can compute probability of y when x is given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can change the formula into shorter version, take P(y) and move it to the left side:\n",
    "\n",
    "$$\\large\\frac{P(y)\\times \\pi_{i=1}^n \\times P(x_{i}|y)}{P(x_{1})\\times P(x_{2})\\times P(x_{3}) ... P(x_{n})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator will be the same for every record, we can consider it as a constant\n",
    "\n",
    "$$\\large P(x_{1})\\times P(x_{2})\\times P(x_{3}) ... P(x_{n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nominator will be directly proportional to this:\n",
    "\n",
    "$$\\large P(y|x_{1}, x_{2}, x_{3}...x_{n})$$    \n",
    "$$\\large \\alpha$$    \n",
    "$$\\large P(y)\\times \\pi_{i=1}^n \\times P(x_{i}|y)$$\n",
    "\n",
    "Since it is directly proportional, in order to find particular value x, we need to take argmax of our computation:\n",
    "\n",
    "$\\large y = argmax(P(y)\\times \\pi_{i=1}^n \\times P(x_{i}|y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example problem statement (binary classification)\n",
    "\n",
    "Consider these three tables below with the outlook, temperature and play. Predict the output whether player will go to play tennis depending on the weather conditions. OUTPUT: Yes or No "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlook\n",
    "|        | YES  | NO   |P(yes)| P(no)|                     \n",
    "|--------|------|------|------|------|\n",
    "|sunny   |  2   |  3   | 2/9  | 3/5  |\n",
    "|overcast|  4   |  0   | 4/9  | 0/5  |\n",
    "|rainy   |  3   |  2   | 3/9  | 2/5  |\n",
    "|total   |  9   |  5   | 100% | 100% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature\n",
    "|        | YES  | NO   |P(yes)| P(no)|\n",
    "|--------|------|------|------|------|\n",
    "|hot     |  2   |  2   | 2/9  | 2/5  |\n",
    "|mild    |  4   |  2   | 4/9  | 2/5  |\n",
    "|cool    |  3   |  1   | 3/9  | 1/5  |\n",
    "|total   |  9   |  5   | 100% | 100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play \n",
    "|        |      |P(yes) & P(no)|\n",
    "|--------|------|--------------|\n",
    "|yes     |  9   |     9/14     | \n",
    "|no      |  5   |     5/14     |\n",
    "|total   |  14  |     100%     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have scenario: today(sunny, hot), where sunny is $x_{1}$ feature and hot is $x_{2}$ feature\n",
    "\n",
    "let's determine whether person will play tennis or not, we can apply formula of naive bayes:\n",
    "\n",
    "What is the probability of YES given today:\n",
    "\n",
    "$\\large P(yes|today) = \\frac{P(sunny|yes)\\times P(hot|yes) \\times P(yes)}{P(today)}$\n",
    "\n",
    "since P(today) is always the same for all records, we can skip it and we get:\n",
    "\n",
    "$2/9 \\times 2/9 \\times 9/14 = 0.031$\n",
    "\n",
    "What is the probability of NO given today:\n",
    "\n",
    "$\\large P(no|today) = \\frac{P(sunny|no)\\times P(hot|no) \\times P(no)}{P(today)} = 3/5 \\times 2/5 \\times 5/14 = 0.08571$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we determine whether the output is yes or no?\n",
    "\n",
    "we have to take the values 0.031 and 0.08571, and normalize it to 1\n",
    "\n",
    "$\\large P(yes) = \\frac{0.031}{0.031+0.08571} \\approx 0.27$\n",
    "\n",
    "$\\large P(no) = 1 - 0.27 = 0.73$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.73 is greater than 0.27, so in this scenario today(sunny, hot) the output will be NO, person will not go to play tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Naive Bayes Classifier with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processed sentences\n",
    "Suppose we have some sentences, for example reviews of some restaurant. We want to predict if the review good or bad.  \n",
    "After NLP pre-processing like removing Stop Words, performing Stemming, BagOfWords, TFIDF or wordToVec we will have someting like this:\n",
    "\n",
    "Sentence 1: The food is delicious\n",
    "\n",
    "Sentence 2: The food is bad\n",
    "\n",
    "Sentence 3: Food is bad\n",
    "\n",
    "Sentence 4: Food delicious\n",
    "\n",
    "Sentence 5: Bad\n",
    "\n",
    "We will get vector representations of words, something like in this table.\n",
    "last column o/p is the output representing good or bad review.\n",
    " \n",
    "|The (f1) |Food (f2) |Delicious (f3)| Bad (f4) |  o/p   |\n",
    "|---------|----------|--------------|----------|--------|\n",
    "| 1       |  1       |    1         |  0       |   1    | \n",
    "| 1       |  1       |    1         |  1       |   0    | \n",
    "| 0       |  1       |    0         |  1       |   0    | \n",
    "| 0       |  1       |    1         |  0       |   1    | \n",
    "| 0       |  0       |    0         |  1       |   0    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying naive bayes formula\n",
    "\n",
    "$\\large P(y = good|sentences)$, where sentences are $\\large [x_{1}, x_{2}, x_{3}, ..., x_{n}]$\n",
    "\n",
    "So, $\\large P(y = good|x_{1} \\times x_{2} \\times x_{3} ... x_{n})$\n",
    "\n",
    "and it directly proportional too\n",
    "\n",
    "$\\large \\alpha$  \n",
    "\n",
    "$\\large P(y=good)\\times \\pi_{i=1}^n \\times P(x_{1}|y=good) \\times P(x_{2}|y=good) \\times ... \\times P(x_{n}|y=good)$\n",
    "\n",
    "For Sentence 1: The food is delicious\n",
    "\n",
    "we have this calculations: \n",
    "\n",
    "= $\\large P(y=good) \\times P(x_{1}|y=good) \\times P(x_{2}|y=good) \\times P(x_{3}|y=good)$\n",
    "\n",
    "= $\\large \\frac{2}{5} \\times \\frac{1}{2} \\times \\frac{2}{4} \\times \\frac{2}{3}$\n",
    "\n",
    "= $\\large \\frac{1}{10} = 0.06$\n",
    "\n",
    "Also with respect to bad:\n",
    "\n",
    "= $\\large P(y=bad) \\times P(x_{1}|y=bad) \\times P(x_{2}|y=bad) \\times P(x_{3}|y=bad)$\n",
    "\n",
    "= $\\large \\frac{3}{5} \\times \\frac{1}{2} \\times \\frac{2}{4} \\times \\frac{1}{3}$\n",
    "\n",
    "= $\\large \\frac{3}{20} = 0.05$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to normalize calculated values to one\n",
    "\n",
    "We got 54% probability for Good\n",
    "\n",
    "$\\large \\frac{0.06}{0.06+0.05} = \\frac{0.06}{0.11} = \\frac{6}{11} = 54.5\\%$\n",
    "\n",
    "and for bad:\n",
    "$\\large 100\\% - 54.5\\% = 45\\%$\n",
    "\n",
    "whichever has a higher probability, it will be the output.\n",
    "\n",
    "So, for the Sentence 1 the output will be good, the sentence given is a good review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Where does this fail?\n",
    "Suppose in our sentence \"The food is delicious\" we have a new word, for example \"The food is delicious and tasty\". \"Tasty\" is the word that is not present in our table. Our value will be zero, and if it is zero, all our multiplication formula will become zero, and by default it will be treated as zero, which is negative output in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
