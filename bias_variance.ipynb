{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Problem statement\n",
    "#### For Regression in our example we will use polynomial linear regression.\n",
    "Problem statement: Suppose we have points spead out in a shape of a curve on a 2-d plane.\n",
    "\n",
    "1. In polynomial linear regression if degree of polynomial is equal to 1, it will act like a simple linear regression, it will try to create the best fit line based on actual points. We know that simple linear regression is not suited for non-linear separated points.  When we compute an error for this model, the error will be high.\n",
    "\n",
    "2. Now, suppose we increase a degree of polynomial to 2 in the polynomial linear regression, what will happen is that the best fit line will become a little bit of a smaller curve in the way that it will satisfy most of the training points and definetly an error will be much smaller.\n",
    "\n",
    "3. Now, let's take one step further, let's increase degree of polynomial to 4. Now this is the condition where every point is exaclty fitted to this particular curve line.\n",
    "\n",
    "*ASIDE NOTE:\n",
    "Variance occurs when the model performs good on the trained dataset but does not do well on a dataset that it is not trained on, like a test dataset or validation dataset. Variance tells us how scattered are the predicted value from the actual value. For easier understanding of the concept, we can take it as test or validation data error.*\n",
    "\n",
    "*Difference in fits between data sets is called* ***Variance***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case for our training dataset the model gives a very high error. In this scenario when the error is very high we call it as **underfitting**. For our training dataset the accuracy will be low and for the test data the accuracy of the model also will be low. In **Underfitting** scenario we have **high bias and low variance**. Our straight line has **high Bias** because it can not capture th curve in the relationship between independent and dependent features, but it has **low variance** because the Sums of Squares are very similar for different data sets (test dataset). In other words we can say that **Bias** is the error of the training data, **variance** is the error of the test data. Overall error will be high in this case. \n",
    "\n",
    "In our last case for our training dataset every single point is perfectly fitted. This scenario is called **overfitting**. That means with respect to training data this particular best fit line satisfies all the points perfectly, but if we have some new points (test points), which are not on the best fit line, the error will be high. For the training dataset the accuracy of the model will be high, however for the test data the accuracy of the model will be low. In the **overfitting** scenario we have **low bias and high variance**. Our squiggly line has **low bias** because it is flexible and can adapt to the curve in the relationship between independent and dependent feature, but it has **high variability** it results vastly different Sums of Squares for different data sets. In other words, for the training data we have small error **low bias**, but high error for test data **high variance**. Overall error will be high in this case.\n",
    "\n",
    "Our main goal should be in such that for the training data our accuracy of the model is high and for the test data or new data is also should be high. And this is achieved by the second case where degree of polynomial is set to 2. Out of 3 scenarios whe should choose the second one where the model has less errors for the training and test data in order to solve our problem statement. This is the most sutable model that we should select. This particular model gives us **low bias and low variance**, since we are getting less error for the training data and for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Problem Statement\n",
    "\n",
    "Suppose we used three models with some hyperparameter tuning techniques to optimize them. In classifcation problem we are trying to compare whether our model is able to do a binary classification like yes or no, and we usually use confusion matrix for that. \n",
    "\n",
    "1. Let's say that our Model 1 gave us training error 1% and test error 20%. What kind of scenario is this? Since our model gives us an error 1% on trainig data, it means **low bias**, and it gives us an error 20% on test data, it means **high variance**. Since we have **low bias and high variance** our scenario is **overfitting** problem.\n",
    "\n",
    "2. Suppose our Model 2 gives an error 25% for training data and 26% for the test data. (If your model has accuracy of 75%, it's not really a good model and needs improvement, but it also depends on domain you are working). So, in this scenario we have training error of 25% **high bias** and test error 26% **low variance**. This scenario is **underfitting** problem.\n",
    "\n",
    "3. In Model 3 our training error is <10% (less than) and our test error is <10% (less than). This is the scenaio where we have less than 10% of error for training data **low bias** and less than 10% of error for test data **low variance**. Since we have **low bias and low variance** this model becomes the most generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "Consider we have decision tree and random forest\n",
    "\n",
    "1. Decision tree by default creates trees by splitting features completely to it's depth. When it does this, this scenario is like **overfitting** condition. Definetly for the training data it will give you a good result, but for the test data this scenario will not work. Basically, this scenario gives us **low bias and high variance**. To improve this we use techniques like decision pruning, we try to create decision tree up to some depth. This is the way actually converting **high variance** to **low variance**. Also there are a lot of hyper parameter tuning techniques.\n",
    "\n",
    "2. another example, let's take random forest. In random forest we use many decision trees in parallel. In this scenario we have **low bias** and initially we have also **high variance** for each descision tree in random forest. This is the property of each decision tree. But since we are combining all these decision trees in parallel, (and random forest is a boostrap aggregation, in boostrap aggregation we take dataset, we give partial number of records to multiple models of decision trees), finally we get the multiple outputs from these models (like 0s and 1s) and based on these outputs we aggregate the final output. Which converts **high variance** to **low variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three commongly used methods for finding the sweet spot between simple model, which has **high bias** (linear regression, Linear Discriminant Analysis and Logistic Regression) and complicated models which has **high variance** (Decision Trees, k-Nearest Neighbors and Support Vector Machines) are: **regularization (Ridge and Lasso Regression), boosting(ADABoost, Gradient Boost), and bagging, also called Bootstrap aggregating (Random Forest)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting and Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Suppose we have a model, input and output. If the model is very good, it will perform good on training and test data set.\n",
    "  - training data, if model gives good accuracy or any other performance metrics (85%, 89%, 95.5% etc.), or test data should have similar\n",
    "  - test data --> accuracy or any other performance metrics (80% - 90%)\n",
    "2. other case scenario is overfitting, because it performs good with train dataset, but performs bad with test dataset:\n",
    "  - training data set -> accuracy is high\n",
    "  - test data set -> accuracy is low\n",
    "3. another case scenario of underfitting, when model performs bad on training and test datasets\n",
    "  - training dataset -> accuracy is low\n",
    "  - test dataset -> accuracy is low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Leakage\n",
    "\n",
    "When our model is trained, it should only know information from our train data. If our model knows some information from test data, it will perform very good for the test data as well. When our model knows information from test data while training the model, it is called **Data Leakage**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
